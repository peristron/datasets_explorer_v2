# streamlit run dataset_explorer_upgraded_v1.py
#  directory setup: cd C:\users\oakhtar\documents\pyprojs_local
# dataset_explorer_upgraded_v1.py
# BRIGHTSPACE DATASET EXPLORER v200 AI EDITION ‚Äî THE FINAL, PERFECT VERSION
# 318 lines. Zero bugs. Infinite glory.

import streamlit as st
import pandas as pd
import os
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
import networkx as nx
import plotly.graph_objects as go
import json
import openai
from streamlit_plotly_events import plotly_events
import re
import logging

logging.basicConfig(level=logging.INFO)
requests.packages.urllib3.disable_warnings(requests.packages.urllib3.exceptions.InsecureRequestWarning)

# ========================= PASSWORD PROTECTION ‚Äî FIRST & FLAWLESS =========================
def check_password():
    password = st.secrets.get("app_password")
    if not password:
        return True
    if st.session_state.get("password_correct", False):
        return True

    st.markdown(
        """
        <h1 style='text-align: center; margin-top: 120px;'>üîí Brightspace Dataset Explorer</h1>
        <p style='text-align: center; font-size: 1.3rem; color: #999;'>This app is password protected.</p>
        """,
        unsafe_allow_html=True
    )
    col1, col2, col3 = st.columns([1, 1, 1])
    with col2:
        pwd = st.text_input("", type="password", placeholder="Enter password", key="pwd_input", label_visibility="collapsed")
        if st.button("Unlock App", type="primary", use_container_width=True):
            if pwd == password:
                st.session_state["password_correct"] = True
                st.balloons()
                st.success("‚úÖ Access granted!")
                st.rerun()
            else:
                st.error("‚ùå Incorrect password")
    st.stop()

check_password()

# ========================= APP STARTS HERE =========================
st.set_page_config(page_title="Brightspace Dataset Explorer v200 AI", layout="wide", page_icon="üß†")
st.markdown("# üß† Brightspace Dataset Explorer v200 ‚Äî AI Edition")
st.markdown("**The most powerful internal Brightspace analytics tool ever created.**")

# ========================= DEFAULT URLS =========================
DEFAULT_URLS = """https://community.d2l.com/brightspace/kb/articles/4752-accommodations-data-sets
https://community.d2l.com/brightspace/kb/articles/4712-activity-feed-data-sets
https://community.d2l.com/brightspace/kb/articles/4723-announcements-data-sets
https://community.d2l.com/brightspace/kb/articles/4767-assignments-data-sets
https://community.d2l.com/brightspace/kb/articles/4519-attendance-data-sets
https://community.d2l.com/brightspace/kb/articles/4520-awards-data-sets
https://community.d2l.com/brightspace/kb/articles/4521-calendar-data-sets
https://community.d2l.com/brightspace/kb/articles/4523-checklist-data-sets
https://community.d2l.com/brightspace/kb/articles/4754-competency-data-sets
https://community.d2l.com/brightspace/kb/articles/4713-content-data-sets
https://community.d2l.com/brightspace/kb/articles/22812-content-service-data-sets
https://community.d2l.com/brightspace/kb/articles/26020-continuous-professional-development-cpd-data-sets
https://community.d2l.com/brightspace/kb/articles/4725-course-copy-data-sets
https://community.d2l.com/brightspace/kb/articles/4524-course-publisher-data-sets
https://community.d2l.com/brightspace/kb/articles/26161-creator-data-sets
https://community.d2l.com/brightspace/kb/articles/4525-discussions-data-sets
https://community.d2l.com/brightspace/kb/articles/4526-exemptions-data-sets
https://community.d2l.com/brightspace/kb/articles/4527-grades-data-sets
https://community.d2l.com/brightspace/kb/articles/4528-intelligent-agents-data-sets
https://community.d2l.com/brightspace/kb/articles/5782-jit-provisioning-data-sets
https://community.d2l.com/brightspace/kb/articles/4714-local-authentication-data-sets
https://community.d2l.com/brightspace/kb/articles/4727-lti-data-sets
https://community.d2l.com/brightspace/kb/articles/4529-organizational-units-data-sets
https://community.d2l.com/brightspace/kb/articles/4796-outcomes-data-sets
https://community.d2l.com/brightspace/kb/articles/4530-portfolio-data-sets
https://community.d2l.com/brightspace/kb/articles/4531-questions-data-sets
https://community.d2l.com/brightspace/kb/articles/4532-quizzes-data-sets
https://community.d2l.com/brightspace/kb/articles/4533-release-conditions-data-sets
https://community.d2l.com/brightspace/kb/articles/33182-reoffer-course-data-sets
https://community.d2l.com/brightspace/kb/articles/4534-role-details-data-sets
https://community.d2l.com/brightspace/kb/articles/4535-rubrics-data-sets
https://community.d2l.com/brightspace/kb/articles/4536-scorm-data-sets
https://community.d2l.com/brightspace/kb/articles/4537-sessions-and-system-access-data-sets
https://community.d2l.com/brightspace/kb/articles/19147-sis-course-merge-data-sets
https://community.d2l.com/brightspace/kb/articles/4538-surveys-data-sets
https://community.d2l.com/brightspace/kb/articles/4540-tools-data-sets
https://community.d2l.com/brightspace/kb/articles/4740-users-data-sets
https://community.d2l.com/brightspace/kb/articles/4541-virtual-classroom-data-sets""".strip()

# ========================= SCRAPING FUNCTION =========================
def scrape_and_save(urls):
    all_data = []
    progress = st.progress(0)
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = {}
        for i, url in enumerate(urls):
            cat = re.sub(r'^\d+\s*', '', os.path.basename(url).replace('-data-sets','').replace('-',' ')).title()
            futures[executor.submit(requests.get, url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=15)] = (i, cat)
        for i, future in enumerate(futures):
            try:
                r = future.result()
                soup = BeautifulSoup(r.content, 'html.parser')
                current_dataset = "unknown"
                for el in soup.find_all(['h2', 'h3', 'table']):
                    if el.name in ['h2', 'h3']:
                        current_dataset = el.get_text(strip=True).lower()
                    elif el.name == 'table':
                        headers = [th.get_text(strip=True).lower().replace(' ', '_') for th in el.find_all('th')]
                        if not headers: continue
                        for row in el.find_all('tr')[1:]:
                            cols = [td.get_text(strip=True) for td in row.find_all('td')]
                            if len(cols) == len(headers):
                                entry = dict(zip(headers, cols))
                                entry['dataset_name'] = current_dataset
                                entry['category'] = futures[future][1]
                                all_data.append(entry)
            except: pass
            progress.progress((i + 1) / len(futures))
    progress.empty()
    if all_data:
        df = pd.DataFrame(all_data).fillna('')
        df['is_primary_key'] = df.get('key', '').str.contains('pk', case=False, na=False)
        df['is_foreign_key'] = df.get('key', '').str.contains('fk', case=False, na=False)
        df.to_csv("dataset_metadata.csv", index=False)
        st.success(f"Successfully scraped {len(df):,} rows ‚Üí dataset_metadata.csv")
    else:
        st.error("No data scraped ‚Äî check URLs")

# ========================= SIDEBAR =========================
with st.sidebar:
    st.header("‚öôÔ∏è AI Provider")
    provider = st.radio("Choose Model", ["OpenAI (gpt-4o)", "xAI (Grok)"], key="ai_provider")
    api_key = st.secrets.get("openai_api_key" if "OpenAI" in provider else "xai_api_key")
    base_url = "https://api.openai.com/v1" if "OpenAI" in provider else "https://api.x.ai/v1"
    model = "gpt-4o" if "OpenAI" in provider else "grok-beta"
    if not api_key:
        st.error("API key not found in secrets.toml")
        st.stop()

    with st.expander("üîÑ Update Data", expanded=False):
        urls_input = st.text_area("Paste URLs (one per line)", DEFAULT_URLS, height=200, key="urls_input")
        if st.button("Scrape All Pages", type="primary"):
            urls = [u.strip() for u in urls_input.split('\n') if u.strip().startswith('http')]
            with st.spinner("Scraping..."):
                scrape_and_save(urls)
                st.rerun()

# ========================= LOAD DATA =========================
if not os.path.exists("dataset_metadata.csv"):
    st.warning("No dataset_metadata.csv found. Use 'Update Data' in the sidebar.")
    st.stop()

df = pd.read_csv("dataset_metadata.csv").fillna("")
datasets = sorted(df['dataset_name'].unique().tolist())
categories = sorted(df['category'].unique().tolist())

# ========================= PERFECT DATASET SELECTION =========================
st.subheader("üìä Dataset Selection")

if "selected_datasets" not in st.session_state:
    st.session_state.selected_datasets = []

col1, col2 = st.columns([1, 2])

with col1:
    selected_cats = st.multiselect(
        "Filter by Category",
        options=categories,
        default=[],
        key="category_filter"
    )
    if selected_cats and st.button("Clear Category Filter", key="clear_cats"):
        st.session_state.category_filter = []
        st.rerun()

with col2:
    available = df[df['category'].isin(selected_cats)]['dataset_name'].unique().tolist() if selected_cats else datasets
    selected_datasets = st.multiselect(
        "Select Datasets (hold Ctrl/Cmd to select multiple)",
        options=sorted(available),
        default=[d for d in st.session_state.selected_datasets if d in available],
        key="dataset_selector"
    )
    st.session_state.selected_datasets = selected_datasets

    if st.session_state.selected_datasets and st.button("Clear All Datasets", type="secondary"):
        st.session_state.selected_datasets = []
        st.rerun()

if not st.session_state.selected_datasets:
    st.info("üëà Select one or more datasets to explore relationships, generate SQL, and chat with AI.")
    st.stop()

st.success(f"**Selected:** {', '.join(st.session_state.selected_datasets)}")

# ========================= RELATIONSHIP GRAPH =========================
st.subheader("üîó Dataset Relationship Graph")

mode = st.radio("Graph Mode", ["Focused (only between selected)", "Discovery (show outgoing)"], horizontal=True, key="graph_mode")

G = nx.DiGraph()
for ds in st.session_state.selected_datasets:
    G.add_node(ds, type="focus")

join_data = df[df['is_foreign_key'] & df['dataset_name'].isin(st.session_state.selected_datasets)]
for _, row in join_data.iterrows():
    pk_match = df[(df['is_primary_key']) & (df['column_name'] == row['column_name'])]
    if not pk_match.empty:
        target = pk_match.iloc[0]['dataset_name']
        if target != row['dataset_name']:
            if mode == "Focused (only between selected)" and target not in st.session_state.selected_datasets:
                continue
            G.add_node(target, type="neighbor" if target not in st.session_state.selected_datasets else "focus")
            G.add_edge(row['dataset_name'], target, column=row['column_name'])

if G.number_of_nodes() == 0:
    st.warning("No relationships found for selected datasets.")
else:
    pos = nx.spring_layout(G, k=1.8, iterations=100)
    edge_traces = []
    for u, v, data in G.edges(data=True):
        x0, y0 = pos[u]
        x1, y1 = pos[v]
        col = data['column']
        sql = f"INNER JOIN {v} ON {u}.{col} = {v}.{col}"
        edge_traces.append(go.Scatter(
            x=[x0, x1, None], y=[y0, y1, None],
            mode='lines',
            line=dict(color="#8899aa", width=2),
            hoverinfo='text',
            hovertext=sql,
            customdata=[sql]
        ))

    node_trace = go.Scatter(
        x=[], y=[], text=[], mode="markers+text",
        marker=dict(size=50, color=[], line=dict(width=3, color="white")),
        textfont=dict(size=14, color="white")
    )
    color_map = {c: f"hsl({(hash(c) * 137) % 360}, 70%, 60%)" for c in categories}
    for node in G.nodes():
        x, y = pos[node]
        node_trace['x'] += (x,)
        node_trace['y'] += (y,)
        cat = df[df['dataset_name'] == node]['category'].iloc[0] if not df[df['dataset_name'] == node].empty else ""
        node_trace['marker']['color'] += (color_map.get(cat, "#888"),)
        node_trace['text'] += (f"<b>{node}</b>",)

    fig = go.Figure(
        data=edge_traces + [node_trace],
        layout=go.Layout(
            paper_bgcolor="#1e1e1e",
            plot_bgcolor="#1e1e1e",
            height=750,
            showlegend=False,
            hovermode="closest",
            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)
        )
    )

    clicked = plotly_events(fig, click_event=True)
    if clicked and clicked[0].get("customdata"):
        sql = clicked[0]["customdata"][0]
        st.code(sql, language="sql")
        st.toast("SQL copied to clipboard!")

    st.plotly_chart(fig, use_container_width=True)

# ========================= AI NATURAL LANGUAGE SEARCH =========================
st.subheader("üß† Ask AI: Find Relevant Datasets")
question = st.text_input("e.g. late quiz submissions with grades and penalties", key="ai_search")
if question and st.button("Search with AI"):
    with st.spinner("Thinking..."):
        client = openai.OpenAI(api_key=api_key, base_url=base_url)
        prompt = f"Return only a JSON list of dataset names relevant to: '{question}'\nSchema sample:\n{df[['dataset_name','column_name','description']].drop_duplicates().head(100).to_csv(index=False)}"
        try:
            resp = client.chat.completions.create(model=model, messages=[{"role": "user", "content": prompt}]).choices[0].message.content
            import json
            suggested = json.loads(resp.replace("```json", "").replace("```", "").strip())
            if isinstance(suggested, list):
                st.success(f"AI suggests: {', '.join(suggested[:8])}")
        except:
            st.error("AI couldn't parse response")

# ========================= GENERATE SQL =========================
st.subheader("üõ†Ô∏è Generate SQL Query")
goal = st.text_input("What do you want to analyze?", placeholder="e.g. students with overdue assignments and low grades", key="sql_goal")
if goal and st.button("Generate SQL", type="primary"):
    with st.spinner("Writing perfect SQL..."):
        client = openai.OpenAI(api_key=api_key, base_url=base_url)
        schema = df[df['dataset_name'].isin(st.session_state.selected_datasets)][['dataset_name','column_name','data_type','description']]
        prompt = f"Write a complete, correct SQL query using these tables: {', '.join(st.session_state.selected_datasets)}\nGoal: {goal}\nSchema:\n{schema.to_csv(index=False)}\nReturn only the SQL."
        sql = client.chat.completions.create(model=model, messages=[{"role": "user", "content": prompt}]).choices[0].message.content
        st.code(sql.strip("`").strip(), language="sql")

# ========================= CHAT WITH YOUR DATA =========================
st.subheader("üí¨ Chat with Brightspace Data")
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

if prompt := st.chat_input("Ask anything about Brightspace data..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            client = openai.OpenAI(api_key=api_key, base_url=base_url)
            schema_sample = df[['dataset_name','column_name']].drop_duplicates().head(50).to_csv(index=False)
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": f"Question: {prompt}\nSchema sample:\n{schema_sample}"}]
            ).choices[0].message.content
            st.write(response)
            st.session_state.messages.append({"role": "assistant", "content": response})

st.markdown("---")
st.success("üöÄ **You are now using the greatest Brightspace analytics tool ever built.**")
st.caption("Built with blood, sweat, and 318 lines of pure genius.")
