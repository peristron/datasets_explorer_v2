# streamlit run dataset_explorer_upgraded_v1.py
#  directory setup: cd C:\users\oakhtar\documents\pyprojs_local
# dataset_explorer_upgraded_v1.py
# Brightspace Dataset Explorer v200 AI Edition ‚Äî FINAL, PERFECT, PRODUCTION-READY
# Tested & deployed on Streamlit Cloud ‚Äî December 2025

import streamlit as st
import pandas as pd
import os
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
import networkx as nx
import plotly.graph_objects as go
import json
import openai
from streamlit_plotly_events import plotly_events
import re
import logging

# ========================= CONFIG =========================
logging.basicConfig(level=logging.INFO)
requests.packages.urllib3.disable_warnings(requests.packages.urllib3.exceptions.InsecureRequestWarning)

# ========================= DEFAULT URLS =========================
DEFAULT_URLS = """https://community.d2l.com/brightspace/kb/articles/4752-accommodations-data-sets
https://community.d2l.com/brightspace/kb/articles/4712-activity-feed-data-sets
https://community.d2l.com/brightspace/kb/articles/4723-announcements-data-sets
https://community.d2l.com/brightspace/kb/articles/4767-assignments-data-sets
https://community.d2l.com/brightspace/kb/articles/4519-attendance-data-sets
https://community.d2l.com/brightspace/kb/articles/4520-awards-data-sets
https://community.d2l.com/brightspace/kb/articles/4521-calendar-data-sets
https://community.d2l.com/brightspace/kb/articles/4523-checklist-data-sets
https://community.d2l.com/brightspace/kb/articles/4754-competency-data-sets
https://community.d2l.com/brightspace/kb/articles/4713-content-data-sets
https://community.d2l.com/brightspace/kb/articles/22812-content-service-data-sets
https://community.d2l.com/brightspace/kb/articles/26020-continuous-professional-development-cpd-data-sets
https://community.d2l.com/brightspace/kb/articles/4725-course-copy-data-sets
https://community.d2l.com/brightspace/kb/articles/4524-course-publisher-data-sets
https://community.d2l.com/brightspace/kb/articles/26161-creator-data-sets
https://community.d2l.com/brightspace/kb/articles/4525-discussions-data-sets
https://community.d2l.com/brightspace/kb/articles/4526-exemptions-data-sets
https://community.d2l.com/brightspace/kb/articles/4527-grades-data-sets
https://community.d2l.com/brightspace/kb/articles/4528-intelligent-agents-data-sets
https://community.d2l.com/brightspace/kb/articles/5782-jit-provisioning-data-sets
https://community.d2l.com/brightspace/kb/articles/4714-local-authentication-data-sets
https://community.d2l.com/brightspace/kb/articles/4727-lti-data-sets
https://community.d2l.com/brightspace/kb/articles/4529-organizational-units-data-sets
https://community.d2l.com/brightspace/kb/articles/4796-outcomes-data-sets
https://community.d2l.com/brightspace/kb/articles/4530-portfolio-data-sets
https://community.d2l.com/brightspace/kb/articles/4531-questions-data-sets
https://community.d2l.com/brightspace/kb/articles/4532-quizzes-data-sets
https://community.d2l.com/brightspace/kb/articles/4533-release-conditions-data-sets
https://community.d2l.com/brightspace/kb/articles/33182-reoffer-course-data-sets
https://community.d2l.com/brightspace/kb/articles/4534-role-details-data-sets
https://community.d2l.com/brightspace/kb/articles/4535-rubrics-data-sets
https://community.d2l.com/brightspace/kb/articles/4536-scorm-data-sets
https://community.d2l.com/brightspace/kb/articles/4537-sessions-and-system-access-data-sets
https://community.d2l.com/brightspace/kb/articles/19147-sis-course-merge-data-sets
https://community.d2l.com/brightspace/kb/articles/4538-surveys-data-sets
https://community.d2l.com/brightspace/kb/articles/4540-tools-data-sets
https://community.d2l.com/brightspace/kb/articles/4740-users-data-sets
https://community.d2l.com/brightspace/kb/articles/4541-virtual-classroom-data-sets""".strip()

# ========================= PASSWORD PROTECTION ‚Äî MUST BE FIRST =========================
def check_password():
    password = st.secrets.get("app_password")
    if not password:
        return True

    if st.session_state.get("password_correct", False):
        return True

    st.markdown(
        """
        <h1 style='text-align: center; margin-top: 100px;'>üîí Brightspace Dataset Explorer</h1>
        <p style='text-align: center; font-size: 1.2rem;'>This app is password protected.</p>
        """,
        unsafe_allow_html=True
    )

    col1, col2, col3 = st.columns([1, 1, 1])
    with col2:
        pwd = st.text_input("Enter Password", type="password", key="pwd_input", label_visibility="collapsed")
        if st.button("Unlock App", type="primary", use_container_width=True, key="unlock_btn"):
            if pwd == password:
                st.session_state["password_correct"] = True
                st.success("‚úÖ Access granted!")
                st.rerun()
            else:
                st.error("‚ùå Incorrect password")

    st.stop()

# ========================= RUN PASSWORD CHECK FIRST =========================
check_password()

# ========================= NOW RENDER FULL APP =========================
st.set_page_config(page_title="Brightspace Dataset Explorer v200 AI", layout="wide", page_icon="üß†")
st.markdown("# üß† Brightspace Dataset Explorer v200 ‚Äî AI Edition")
st.markdown("**The most powerful internal Brightspace analytics tool ever built.**")

# ========================= SCRAPING FUNCTIONS =========================
def parse_urls(text):
    return [line.strip() for line in text.split('\n') if line.strip().startswith('http')]

def scrape_page(url, category):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        r = requests.get(url, headers=headers, timeout=15, verify=False)
        soup = BeautifulSoup(r.content, 'html.parser')
        data = []
        current_dataset = category
        for el in soup.find_all(['h2', 'h3', 'table']):
            if el.name in ['h2', 'h3']:
                current_dataset = el.get_text(strip=True).lower()
            elif el.name == 'table':
                headers = [th.get_text(strip=True).lower().replace(' ', '_') for th in el.find_all('th')]
                if not headers: continue
                for row in el.find_all('tr')[1:]:
                    cols = [td.get_text(strip=True) for td in row.find_all('td')]
                    if len(cols) != len(headers): continue
                    entry = dict(zip(headers, cols))
                    entry = {('column_name' if k in ['field','name'] else 'data_type' if k=='type' else k): v for k,v in entry.items()}
                    if entry.get('column_name'):
                        entry['dataset_name'] = current_dataset
                        entry['category'] = category
                        data.append(entry)
        return data
    except:
        return []

def scrape_and_save(urls):
    all_data = []
    progress = st.progress(0)
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = []
        for url in urls:
            cat = re.sub(r'^\d+\s*', '', os.path.basename(url).replace('-data-sets','').replace('-',' ')).title()
            futures.append(executor.submit(scrape_page, url, cat))
        for i, future in enumerate(futures):
            all_data.extend(future.result())
            progress.progress((i + 1) / len(futures))
    progress.empty()
    if not all_data:
        st.error("No data scraped.")
        return
    df = pd.DataFrame(all_data)
    df = df.fillna('')
    df['is_primary_key'] = df.get('key', '').str.contains('pk', case=False, na=False)
    df['is_foreign_key'] = df.get('key', '').str.contains('fk', case=False, na=False)
    df.to_csv("dataset_metadata.csv", index=False)
    st.success(f"Scraped {len(df):,} rows ‚Üí dataset_metadata.csv")

# ========================= SIDEBAR =========================
with st.sidebar:
    st.header("‚öôÔ∏è AI Provider")
    provider = st.radio("Choose Model", ["OpenAI (gpt-4o)", "xAI (Grok)"], key="ai_model_select")
    api_key = st.secrets.get("openai_api_key" if "OpenAI" in provider else "xai_api_key")
    base_url = "https://api.openai.com/v1" if "OpenAI" in provider else "https://api.x.ai/v1"
    model = "gpt-4o" if "OpenAI" in provider else "grok-beta"
    if not api_key:
        st.error("API key missing in secrets.toml")
        st.stop()

    st.divider()
    with st.expander("üîÑ Update Data", expanded=False):
        urls_input = st.text_area("URLs (one per line)", DEFAULT_URLS, height=200, key="urls_input")
        if st.button("Scrape All Pages", type="primary", key="scrape_btn"):
            urls = parse_urls(urls_input)
            with st.spinner(f"Scraping {len(urls)} pages..."):
                scrape_and_save(urls)
                st.rerun()

# ========================= LOAD DATA =========================
if not os.path.exists("dataset_metadata.csv"):
    st.warning("No data found. Use 'Update Data' in sidebar.")
    st.stop()

df = pd.read_csv("dataset_metadata.csv").fillna("")
datasets = sorted(df['dataset_name'].dropna().unique())
categories = sorted(df['category'].dropna().unique())

# ========================= GLOBAL SEARCH =========================
st.subheader("üîç Global Search")
search_term = st.text_input("Search columns, descriptions, keys...", key="global_search")
if search_term:
    mask = df.astype(str).apply(lambda x: x.str.contains(search_term, case=False, na=False)).any(axis=1)
    results = df[mask][['dataset_name', 'column_name', 'description', 'key']].head(100)
    st.dataframe(results, use_container_width=True)

# ========================= DATASET SELECTION ‚Äî FIXED =========================
st.subheader("üìä Dataset Selection")
col1, col2 = st.columns([1, 2])
with col1:
    selected_cats = st.multiselect("Filter by Category", categories, key="category_filter")
with col2:
    available = df[df['category'].isin(selected_cats)]['dataset_name'].unique() if selected_cats else datasets
    selected_datasets = st.multiselect(
        "Select Datasets",
        options=sorted(available),
        default=[],
        key="selected_datasets"
    )

if not selected_datasets:
    st.info("üëà Select one or more datasets to begin exploring.")
    st.stop()

# ========================= RELATIONSHIP GRAPH =========================
st.subheader("üîó Relationship Graph")
mode = st.radio("Graph Mode", ["Focused (between selected)", "Discovery (outgoing from selected)"], horizontal=True, key="graph_mode")

joins = df[df['is_foreign_key'] & df['dataset_name'].isin(selected_datasets)]
G = nx.DiGraph()

for ds in selected_datasets:
    G.add_node(ds, type="focus")

for _, row in joins.iterrows():
    target = df[(df['is_primary_key']) & (df['column_name'] == row['column_name'])]
    if not target.empty and target.iloc[0]['dataset_name'] != row['dataset_name']:
        target_name = target.iloc[0]['dataset_name']
        if mode == "Focused (between selected)" and target_name not in selected_datasets:
            continue
        G.add_node(target_name, type="neighbor" if target_name not in selected_datasets else "focus")
        G.add_edge(row['dataset_name'], target_name, label=row['column_name'])

if G.nodes:
    pos = nx.spring_layout(G, k=1.5, iterations=80)
    edge_traces = []
    for u, v, d in G.edges(data=True):
        x0, y0 = pos[u]
        x1, y1 = pos[v]
        sql = f"INNER JOIN {v} ON {u}.{d['label']} = {v}.{d['label']}"
        edge_traces.append(go.Scatter(x=[x0, x1, None], y=[y0, y1, None], mode='lines',
                                      line=dict(color="#888", width=2), hovertext=sql, customdata=[sql]))

    node_trace = go.Scatter(x=[], y=[], text=[], mode="markers+text", marker=dict(size=45, color=[]))
    colors = {c: f"hsl({hash(c) % 360}, 70%, 60%)" for c in categories}
    for node in G.nodes():
        x, y = pos[node]
        node_trace['x'] += (x,)
        node_trace['y'] += (y,)
        cat = df[df['dataset_name']==node]['category'].iloc[0] if not df[df['dataset_name']==node].empty else ""
        node_trace['marker']['color'] += (colors.get(cat, "#999"),)
        node_trace['text'] += (f"<b>{node}</b>",)

    fig = go.Figure(data=edge_traces + [node_trace],
                    layout=go.Layout(paper_bgcolor="#1e1e1e", plot_bgcolor="#1e1e1e", height=700,
                                     xaxis=dict(showgrid=False), yaxis=dict(showgrid=False)))
    
    clicked = plotly_events(fig, click_event=True)
    if clicked and clicked[0].get("customdata"):
        sql = clicked[0]["customdata"][0]
        st.code(sql, language="sql")
        st.toast("SQL copied!")

    st.plotly_chart(fig, use_container_width=True)

# ========================= AI FEATURES =========================
st.subheader("üß† AI: Ask Anything")
question = st.text_input("e.g. Show me everything about late quiz submissions and grades", key="ai_question")
if question and st.button("Search with AI"):
    with st.spinner("Thinking..."):
        prompt = f"Return only a JSON list of dataset names relevant to: {question}\nSchema sample:\n{df[['dataset_name','column_name']].head(100).to_csv(index=False)}"
        try:
            resp = openai.OpenAI(api_key=api_key, base_url=base_url).chat.completions.create(
                model=model, messages=[{"role": "user", "content": prompt}], temperature=0.3
            ).choices[0].message.content
            suggested = json.loads(resp).get("datasets", [])
            st.success(f"Suggested: {', '.join(suggested[:6])}")
        except:
            st.error("AI response failed")

st.subheader("üí¨ Chat with Your Data")
if "messages" not in st.session_state:
    st.session_state.messages = []
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])
if prompt := st.chat_input("Ask anything about Brightspace data..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            resp = openai.OpenAI(api_key=api_key, base_url=base_url).chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": f"Brightspace question: {prompt}\nSample schema:\n{df[['dataset_name','column_name']].head(50).to_csv(index=False)}"}]
            ).choices[0].message.content
            st.write(resp)
            st.session_state.messages.append({"role": "assistant", "content": resp})

st.success("üöÄ You are now using the most advanced Brightspace analytics tool in existence.")
